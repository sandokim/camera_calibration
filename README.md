## ì»¤ìŠ¤í…€ ì¹´ë©”ë¼ ë³´ì • ë° ìì„¸ ì¶”ì • í”„ë¡œì„¸ìŠ¤ 

### 1. ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì • (`camera_calibration_intrinsics.py`)
- **ì‹¤í–‰ ë¹ˆë„:** ì¹´ë©”ë¼ë‹¹ í•œ ë²ˆë§Œ ì‹¤í–‰
- **ì…ë ¥:** ì¹´ë©”ë¼ë‹¹ 20ì¥ ì´ìƒì˜ ì²´ì»¤ë³´ë“œ ì´ë¯¸ì§€
- **ëª©ì :** ì¹´ë©”ë¼ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°(ì´ˆì  ê±°ë¦¬, ê´‘í•™ ì¤‘ì‹¬, ì™œê³¡ ê³„ìˆ˜)ë¥¼ êµ¬í•¨
- **ì¶œë ¥:** ê° ì¹´ë©”ë¼ë³„ë¡œ intrinsics.json íŒŒì¼ ìƒì„±

### 2. ì¹´ë©”ë¼ ì™¸ë¶€íŒŒë¼ë¯¸í„° ì¶”ì • (`camera_calibration_extrinsics.py`)
- **ì‹¤í–‰ ë¹ˆë„:** ë°ì´í„°ì…‹ êµ¬ì¶• ì‹œë§ˆë‹¤ ì‹¤í–‰
- **ì…ë ¥:** ë©€í‹° ì¹´ë©”ë¼ ì„¸íŒ…ì—ì„œ ë™ì‹œ ì´¬ì˜ëœ ì²´ì»¤ë³´ë“œ ì´ë¯¸ì§€ ìµœì†Œ 20ì¥ ì´ìƒ
- **ëª©ì :** ê³µí†µëœ í•˜ë‚˜ì˜ ì¢Œí‘œê³„ì—ì„œ ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„°(ì¹´ë©”ë¼ í¬ì¦ˆ) ì¶”ì •
- **ì¶œë ¥:**
  - PnP ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì²´ì»¤ë³´ë“œ ì¢Œí‘œê³„(world frame)ì—ì„œ ì¹´ë©”ë¼ ì¢Œí‘œê³„(camera frame)ë¡œì˜ ë³€í™˜ í–‰ë ¬ ê³„ì‚°
  - cam0ë¥¼ referenceë¡œ ì‚¼ê³  ë‚˜ë¨¸ì§€ ì¹´ë©”ë¼ë“¤ì˜ ì™¸ë¶€ íŒŒë¼ë¯¸í„° ê³„ì‚°
  - extrinsics.jsonìœ¼ë¡œ ì¹´ë©”ë¼ í¬ì¦ˆ ì €ì¥ 

checkerboardë¡œ êµ¬í•œ extrinsicsì™€ intrinsicsë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë³µì‚¬í•˜ê³ ,
scene/face/images/camXì— ë“¤ì–´ìˆëŠ” imageì˜ ì´ë¦„ì„ extrinsics.jsonì— ë§ê²Œ ìˆ˜ì •

```python
+â”€â”€ scene/myface/images


+â”€â”€ scene/myface
â”‚   +â”€â”€ extrinsics.json
â”‚   +â”€â”€ images
â”‚   â”‚   +â”€â”€ cam0
â”‚   â”‚   â”‚   +â”€â”€ cam0_img0.jpg
â”‚   â”‚   â”‚   +â”€â”€ intrinsics.json
â”‚   â”‚   +â”€â”€ cam1
â”‚   â”‚   â”‚   +â”€â”€ cam1_img0.jpg
â”‚   â”‚   â”‚   +â”€â”€ intrinsics.json
â”‚   â”‚   +â”€â”€ cam2
â”‚   â”‚   â”‚   +â”€â”€ cam2_img0.jpg
â”‚   â”‚   â”‚   +â”€â”€ intrinsics.json
â”‚   â”‚   +â”€â”€ cam3
â”‚   â”‚   â”‚   +â”€â”€ cam3_img0.jpg
â”‚   â”‚   â”‚   +â”€â”€ intrinsics.json

+â”€â”€ scene/4_camera_calib_data
â”‚   +â”€â”€ checkerboard
â”‚   â”‚   +â”€â”€ cam0
â”‚   â”‚   â”‚   +â”€â”€ cam0_checkerboard_img0.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam0_checkerboard_img1.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam0_checkerboard_img2.jpg
â”‚   â”‚   â”‚   ...
â”‚   â”‚   â”‚   +â”€â”€ intrinsics.json
â”‚   â”‚   +â”€â”€ cam1
â”‚   â”‚   â”‚   +â”€â”€ cam1_checkerboard_img0.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam1_checkerboard_img1.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam1_checkerboard_img2.jpg
â”‚   â”‚   â”‚   ...
â”‚   â”‚   â”‚   +â”€â”€ intrinsics.json
â”‚   â”‚   +â”€â”€ cam2
â”‚   â”‚   â”‚   +â”€â”€ cam2_checkerboard_img0.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam2_checkerboard_img1.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam2_checkerboard_img2.jpg
â”‚   â”‚   â”‚   ...
â”‚   â”‚   â”‚   +â”€â”€ intrinsics.json
â”‚   â”‚   +â”€â”€ cam3
â”‚   â”‚   â”‚   +â”€â”€ cam3_checkerboard_img0.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam3_checkerboard_img1.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam3_checkerboard_img2.jpg
â”‚   â”‚   â”‚   ...
â”‚   â”‚   â”‚   +â”€â”€ intrinsics.json
â”‚   â”‚   +â”€â”€ multicapture
â”‚   â”‚   â”‚   +â”€â”€ cam0
â”‚   â”‚   â”‚   â”‚   +â”€â”€ cam0_checkerborard_img_x.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam1
â”‚   â”‚   â”‚   â”‚   +â”€â”€ cam1_checkerborard_img_x.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam2
â”‚   â”‚   â”‚   â”‚   +â”€â”€ cam2_checkerborard_img_x.jpg
â”‚   â”‚   â”‚   +â”€â”€ cam3
â”‚   â”‚   â”‚   â”‚   +â”€â”€ cam3_checkerborard_img_x.jpg
â”‚   â”‚   +â”€â”€ extrinsics.json
â”‚   â”‚   +â”€â”€ other_scene_images
```


### 3. COLMAP í˜•íƒœì˜ cameras.txt, images.txtë¡œ ì €ì¥
- **ì…ë ¥:** intrinsics.json, extrinsics.json
- **ëª©ì :** COLMAP í˜•íƒœì˜ ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ì €ì¥
- **ì¶œë ¥:** cameras.txt, images.txt, points3D.txt(empty)


### 4. [sparse/0/ í´ë” ìƒì„± í›„ 3.ì—ì„œ ì¶œë ¥í•œ cameras.txt, images.txt, points3D.txt(empty)ë¥¼ ë³µì‚¬](https://colmap.github.io/faq.html#reconstruct-sparse-dense-model-from-known-camera-poses)
If the camera poses are known and you want to reconstruct a sparse or dense model of the scene, you must first manually construct a sparse model by creating a `cameras.txt, points3D.txt`, and `images.txt` under a new folder:

`python convert_to_COLMAP_fmt.py`ë¥¼ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í´ë”ê°€ ìƒì„±ë˜ê³ , triangulationì´ ìˆ˜í–‰ë˜ì–´, `triangulated/sparse/0` í´ë”ê°€ ìƒì„±ë¨
COLMAPì˜ `model_convert`ë¥¼ í†µí•´ `triangulated/sparse/0` í´ë”ì— ìƒì„±ëœ `bin` íŒŒì¼ì„ `txt` íŒŒì¼ í˜•ì‹ìœ¼ë¡œë„ ë³€í™˜í•˜ì—¬ ì €ì¥

```python
+â”€â”€ manually/created/sparse/model
â”‚   +â”€â”€ cameras.txt
â”‚   +â”€â”€ images.txt
â”‚   +â”€â”€ points3D.txt
+â”€â”€ triangulated/sparse/0
â”‚   +â”€â”€ cameras.txt
â”‚   +â”€â”€ images.txt
â”‚   +â”€â”€ points3D.txt
â”‚   +â”€â”€ cameras.bin
â”‚   +â”€â”€ images.bin
â”‚   +â”€â”€ points3D.bin
```

The points3D.txt file should be empty while every other line in the images.txt should also be empty, since the sparse features are computed, as described below. You can refer to this article for more information about the structure of a sparse model.

#### COLMAPì—ì„œ Import Modelë¡œ sparse/0/ í´ë” ì„ íƒí•˜ì˜€ìœ¼ë‚˜ COLMAP Processing > Database managementì—ì„œ Camerasì™€ Imagesê°€ ëª¨ë‘ ë¹„ì–´ìˆìŒ

COLMAPì€ .txt íŒŒì¼ ê¸°ë°˜ì˜ ëª¨ë¸ì„ GUIì—ì„œ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  database.dbë¥¼ ì§ì ‘ êµ¬ì„±í•˜ê±°ë‚˜ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìœ ì¼í•œ ë°©ë²•ì…ë‹ˆë‹¤. GUI/CLI ëª¨ë‘ ë™ì¼í•˜ê²Œ database.db â†’ ëª¨ë“  ì´ë¯¸ì§€, ì¹´ë©”ë¼, feature ì •ë³´ ì‚¬ìš©

The image reader can only take the parameters for a single camera. If you want to specify the parameters for multiple cameras, you would have to modify the SQLite database directly. This should be easy by modifying the `scripts/python/database.py` script.

#### Question: How to format cameras.txt for Reconstruct sparse/dense model from known camera poses #428 
[different cameras with different intrinsics, multiple camerasì˜ parametersë¥¼ ì§€ì •í•˜ë ¤ë©´ SQLite databaseë¥¼ ì§ì ‘ ìˆ˜ì •í•´ì•¼í•˜ê³ , ì´ëŠ” `colmap/scripts/python/database.py` ìŠ¤í¬ë¦½íŠ¸ë¡œ ê°€ëŠ¥í•˜ë‹¤.](https://github.com/colmap/colmap/issues/428)

#### gaussian_splattingì„ ìœ„í•´ì„  convert_to_COLMAP_fmt.pyì—ì„œ OpenCV ì¹´ë©”ë¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ê¸°ì—, intrinsicsë¥¼ ì‚¬ìš©í•˜ì—¬, undistortioní•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼ í•¨
- undistortionì´ ì•„ì§ ì•ˆëœ ì´ë¯¸ì§€ë“¤ì„ -> `multicam/build/Desktop_Qt_6_9_0_MSVC2022_64bit-Release/scene/myface_undistort/input` ì— ë„£ê¸°
- `multicam/build/Desktop_Qt_6_9_0_MSVC2022_64bit-Release/scene/myface_undistort/distorted` í´ë” ì•ˆì— `database.db`ì™€ `triangulated/sparse/0` í´ë”ë¥¼ ë³µì‚¬
- `triangulated/sparse/0`ì—ì„œ ì–»ì—ˆë˜ `cameras.txt, images.txt, points3D.txt` -> `distorted/sparse/0`ì— ë³µì‚¬í–ˆìŒ

```python
<location>
|---input
|   |---<image 0>
|   |---<image 1>
|   |---...
|---distorted
    |---database.db
    |---sparse
        |---0
            |---...
```

- `convert.py`ë¥¼ ì‹¤í–‰í•˜ë©´ undistortedëœ ì´ë¯¸ì§€ë“¤ì€ `images` í´ë”ì— ìƒì„±ë¨

```python
<location>
â”œâ”€â”€ input/                   # (1) ì›ë³¸ ì´ë¯¸ì§€ (ì™œê³¡ í¬í•¨)
â”‚   â”œâ”€â”€ image0.jpg
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ distorted/              # (2) COLMAP ì‘ì—… ê³µê°„ (ì™œê³¡ í¬í•¨)
â”‚   â”œâ”€â”€ database.db         # COLMAPìš© DB (ì™œê³¡ í¬í•¨ ì´ë¯¸ì§€ ê¸°ì¤€)
â”‚   â””â”€â”€ sparse/
â”‚       â””â”€â”€ 0/
â”‚           â””â”€â”€ cameras.bin
â”‚           â””â”€â”€ images.bin
â”‚           â””â”€â”€ points3D.bin
â”œâ”€â”€ images/                 # (3) Undistorted ì´ë¯¸ì§€ (Pinhole ê¸°ì¤€ ë³€í™˜ë¨)
â”œâ”€â”€ sparse/                 # (4) Undistorted ê¸°ì¤€ ì¬êµ¬ì„± ê²°ê³¼
â”‚   â””â”€â”€ 0/
â”‚       â””â”€â”€ cameras.bin
â”‚       â””â”€â”€ images.bin
â”‚       â””â”€â”€ points3D.bin
```

- ë¯¸ë¦¬ COLMAPì˜ feature extraction, feature matchingì„ ì‹¤í–‰í•˜ì—¬, `database.db`ì— ì €ì¥í–ˆì—ˆê¸° ë•Œë¬¸ì—, `convert.py`ì—ì„œëŠ” feature extraction, feature matchingì„ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ -> `convert.py`ì—ì„œ skip matchingì„ í•˜ë©´ feature extraction, feature matching, mapper((SfM & triangulation) + bundle adjustment)ì„ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
- ë³¸ì¸ì€ `convert_to_COLMAP_fmt.py`ì—ì„œ feature extraction, feature matchingì„ ìˆ˜í–‰í•˜ê³ , ë¯¸ë¦¬ PnP ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ê³„ì‚°í•œ ì¹´ë©”ë¼ í¬ì¦ˆë¥¼ ì£¼ì—ˆê³ , ì´ ì¹´ë©”ë¼ í¬ì¦ˆë¥¼ ì‚¬ìš©í•˜ì—¬ point triangulatorë¥¼ ìˆ˜í–‰í•˜ì˜€ì§€ë§Œ, bundle adjustmentëŠ” ì‹¤í–‰í•˜ì§€ ì•Šì•˜ìŒ -> bundle adjustmentë„ ì¶”ê°€í•˜ì
  <p align="center">
    <img src="https://github.com/user-attachments/assets/90ac44df-36aa-4389-8d69-41d78d6ba8b5" width="600"/>
  </p>
  <p align="center"><em>Structure-from-Motion (SfM) Pipeline : [Source](http://theia-sfm.org/sfm.html)</em></p>
- Traditional SfM pipelines such as COLMAP, operate by taking a sequence of images, detecting features points and descriptors images and matching these features across different views. RANSAC is then used to filter out bad matches while maximizing inliers. Using triangulation, camera poses are estimated and iteratively refined using Bundle Adjustment (BA) with an objective to minimize reprojection errors.

###  Limitations of Traditional SfM:
- Traditional SfM methods require the camera intrinsics to be known beforehand limiting their applicability in scenarios where camera parameters are unknown or missing. **These methods typically consider only a handful of keypoints to obtain sparse point clouds discarding the global geometric context of the scene.** They are often time consuming and complex, involving multiple intermediate stages potentially introducing noise. They donâ€™t handle scenes with low texture or repetitive patters well. **Bundle adjustment, a key step in refining 3D points is computationally intensive especially for larger scenes.** They require highly overlapping image sequences and camera motion for accurate reconstruction.
- SfMì€ ì˜¤ì§ ëª‡ê°œì˜ keypointsë§Œ ê³ ë ¤í•´ì„œ sparse point cloudsë¥¼ ì–»ê¸° ë•Œë¬¸ì— sceneì— ëŒ€í•œ global geometric context ì •ë³´ë¥¼ ë¬´ì‹œí•œë‹¤.
- SfMì—ì„œ bundle adjustmentëŠ” 3D pointsë¥¼ refineí•˜ëŠ”ë° í•µì‹¬ì ì¸ ì—°ì‚°ì´ì§€ë§Œ, computationally intensiveí•˜ë‹¤.
- DUSt3Rì´ single forward passë¡œ traditional SfMì˜ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ ì—†ì´ ì¢‹ì€ estimateë¥¼ ì£¼ì—ˆì§€ë§Œ, DUSt3Rì€ ë¶€ì •í™•í•œ global SfM reconstructionì´ë‹¤.
- MASt3Rì´ DUSt3Rì—ì„œ ë°œì „ì‹œì¼œ matching image pairsì— ê°•ì¸í•˜ê²Œ ë””ìì¸ ë˜ì—ˆì§€ë§Œ larger scenesì— ëŒ€í•´ì„œëŠ” scaleí•˜ì§€ ëª»í•œë‹¤.
- MASt3R-SfMì€ COLMAP-SfMì˜ Bundle Adjustmentë¥¼ ì œê±°í•´ì„œ ì—°ì‚° ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•˜ì˜€ê³ , Gaussian Splattingì— initialì„ ë§Œë“¤ë•Œ ë°”ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.

- BAëŠ” ê¸°ì¡´ì˜ 3D êµ¬ì¡°ì™€ ì¹´ë©”ë¼ í¬ì¦ˆë¥¼ ì •ë°€í•˜ê²Œ ì •í•©í•˜ëŠ” ìµœì í™” ê³¼ì •ì…ë‹ˆë‹¤. **bundle adjustmentì—ì„œëŠ” 2D reprojection errorë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.** Bundle Adjustment(BA)ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•Šìœ¼ë©´ í¬ì¦ˆ ë° 3D êµ¬ì¡° ê°„ì˜ ì •í•© ìµœì í™”ê°€ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŒ. ë‹¨, bundle adjustmentëŠ” feature matchingì—ì„œ ì°¾ì•„ì§„ ëŒ€ì‘ì  ê´€ê³„ê°€ ë¶€ì •í™•í•˜ë©´, triangulationìœ¼ë¡œ ì°¾ì•„ì§„ 3D ì ë“¤ì˜ ìœ„ì¹˜ë„ ë¶€ì •í™•í•´ì§ˆ ìˆ˜ ìˆê³ , bundle adjustment ê²°ê³¼ë„ ë¶€ì •í™•í•´ì§ˆ ìˆ˜ ìˆìŒ. ì¦‰, ì´ˆê¸° ì…ë ¥ê°’(3D ì , ëŒ€ì‘ì , í¬ì¦ˆ)ì´ ë¶€ì •í™•í•˜ê±°ë‚˜ ë¶€ì¡±í•˜ë‹¤ë©´, BAëŠ” ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ëŒ€ì‹ , ì˜ëª»ëœ ë°©í–¥ìœ¼ë¡œ ìˆ˜ë ´í•˜ê±°ë‚˜ ê³¼ì í•©(overfitting)ì˜ í˜•íƒœë¡œ ìˆ˜ë ´í•  ìˆ˜ ìˆìŒ
- 3D ì ì˜ ë°€ë„ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ê²½ìš°, reconstructëœ 3D ì ë“¤ì´ sparseí•˜ê³ , ì¥ë©´ êµ¬ì¡°ë¥¼ ì¶©ë¶„íˆ í¬ê´„í•˜ì§€ ëª»í•˜ë©´ ì¹´ë©”ë¼ í¬ì¦ˆ ê°„ ì œì•½ ì¡°ê±´ì´ ì•½í•´ì§. ëŒ€ì‘ì  ê¸°ë°˜ ì—ëŸ¬ ìµœì†Œí™”ê°€ ì „ì²´ ì¥ë©´ ì •í•©ì„±ìœ¼ë¡œ ì—°ê²°ë˜ì§€ ì•ŠìŒ. ì´ëŸ° ê²½ìš° BAë¥¼ ìˆ˜í–‰í•´ë„ ìµœì í™”ì— ì¶©ë¶„í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬, ê²°ê³¼ê°€ ë¬´ì˜ë¯¸í•˜ê±°ë‚˜ ì˜¤íˆë ¤ ë‚˜ë¹ ì§ˆ ìˆ˜ ìˆìŒ

### convert.pyì—ì„œëŠ” feature extraction, feature matching, mapper ê°€ ìˆ˜í–‰ë˜ì—ˆê³ , mapperëŠ” ì¹´ë©”ë¼ í¬ì¦ˆ ê³„ì‚° (SfM)ê³¼ triangulationì„ ìˆ˜í–‰í•œ í›„ì— (local+global) bundle adjustmentë¥¼ ìˆ˜í–‰í•¨

```
python convert.py -s multicam/build/Desktop_Qt_6_9_0_MSVC2022_64bit-Release/scene/myface_undistort --skip_matching
```

ê° ì¹´ë©”ë¼ë§ˆë‹¤ intrinsicsê°€ ë‹¬ëì–´ì„œ, undistortionëœ ì´ë¯¸ì§€ë“¤ì˜ ì‚¬ì´ì¦ˆê°€ ë‹¤ë¥´ê²Œ ìƒì„±ë¨

## ì¹´ë©”ë¼ ë‚´ë¶€íŒŒë¼ë¯¸í„° ì„¤ì •
- image_undistorterëŠ” ì´ë¯¸ì§€ë¥¼ ideal pinhole camera ëª¨ë¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ, projection ì¤‘ì‹¬(cx, cy) ê¸°ì¤€ìœ¼ë¡œ ìœ íš¨í•œ ì‹œì•¼ ì˜ì—­ë§Œì„ ìœ ì§€í•¨.
- ë”°ë¼ì„œ ì¶”ì •ëœ intrinsicsì˜ cx, cyê°€ ì´ë¯¸ì§€ ì¤‘ì‹¬ì—ì„œ í¬ê²Œ ë²—ì–´ë‚œ ê²½ìš°, warped ì˜ì—­ì´ ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë°€ë ¤ cropì´ ì‹¬í•˜ê²Œ ë°œìƒí•¨
- **camera_calibration_intrinsics.pyì—ì„œ ì²´ì»¤ë³´ë“œ íŒ¨í„´ìœ¼ë¡œ ì¶”ì •í•œ intrinsicsì¸ mtxì—ì„œ ì„ì˜ë¡œ mtx[0,2] = W/2, mtx[1,2] = H/2ë¡œ ëŒ€ì²´í•˜ì—¬ì„œ cropì´ ì‹¬í•˜ê²Œ ë˜ëŠ” ë¶€ë¶„ì´ í•´ê²°ë˜ì—ˆìŒ**
ì¹´ë©”ë¼ì˜ **ë‚´ë¶€ íŒŒë¼ë¯¸í„°(intrinsic parameters)**ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤: 2D Translation Matrix, 2D Scaling Matrix, ê·¸ë¦¬ê³  2D Shear Matrixì…ë‹ˆë‹¤.
- 2D Translation MatrixëŠ” **principal point(ì£¼ì )**ì˜ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ëŠ” ì´ë¯¸ì§€ ì¢Œí‘œê³„ì—ì„œ ê´‘ì¶•ì´ í†µê³¼í•˜ëŠ” ì ì…ë‹ˆë‹¤. **ëŒ€ë¶€ë¶„ì˜ ê²½ìš°, ì´ ì£¼ì ì€ ì´ë¯¸ì§€ì˜ ì¤‘ì‹¬ì— ìœ„ì¹˜í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.**
- 2D Shear MatrixëŠ” ì´ë¯¸ì§€ ì¶• ê°„ì˜ ë¹„ì§êµì„±ì„ í‘œí˜„í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œ ëŒ€ë¶€ë¶„ì˜ ì¹´ë©”ë¼ì—ì„œëŠ” í”½ì…€ì˜ x, yì¶•ì´ ì§êµí•œë‹¤ê³  ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, ì´ í•­ì€ ë¬´ì‹œë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
- 2D Scaling MatrixëŠ” ì´ë¯¸ì§€ì˜ xì¶•ê³¼ yì¶• ë°©í–¥ì— ëŒ€í•œ **focal length(ì´ˆì  ê±°ë¦¬)**ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ˆì  ê±°ë¦¬ëŠ” ì´ë¯¸ì§€ ì„¼ì„œì™€ ì´ë¯¸ì§€ í‰ë©´ ì‚¬ì´ì˜ ê±°ë¦¬ë¡œ í•´ì„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ê°’ì€ ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì—ì„œ í•µì‹¬ì ìœ¼ë¡œ ì¶”ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.
- ë”°ë¼ì„œ ì‹¤ì œ ì‘ìš©ì´ë‚˜ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” intrinsic parameters ì¤‘ focal lengthì— í•´ë‹¹í•˜ëŠ” scaling ìš”ì†Œë§Œì„ ì¶”ì • ëŒ€ìƒìœ¼ë¡œ ì‚¼ê³  ìˆìœ¼ë©°, **ë‚˜ë¨¸ì§€ ìš”ì†Œë“¤ì€ ê³ ì •(principal point)**ë˜ê±°ë‚˜ **ë¬´ì‹œ(shear)**ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

## image_undistorter ëª…ë ¹
- ì™œê³¡ ì œê±°ëœ ì´ë¯¸ì§€ì™€ í•¨ê»˜, ì´ ì´ë¯¸ì§€ì— ë§ëŠ” ideal PINHOLE (ë˜ëŠ” SIMPLE_PINHOLE) ì¹´ë©”ë¼ ëª¨ë¸ì„ ìƒì„±í•¨
- 'image undistorter' ëª…ë ¹ì„ ìˆ˜í–‰í•œë‹¤ê³  í•´ì„œ database.db ë‚´ OpenCV ëª¨ë¸ì´ PINHOLEë¡œ ìë™ ë³€í™˜ë˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
- ëŒ€ì‹ , undistorted ê²°ê³¼ì— ëŒ€ì‘í•˜ëŠ” PINHOLE ëª¨ë¸ì´ ë³„ë„ íŒŒì¼(cameras.txt ë“±)ì— ìƒì„±ë  ë¿ì…ë‹ˆë‹¤.

### poses_bounds.npy ìƒì„±
```python
git clone https://github.com/Fyusion/LLFF
python LLFF/imgs2poses.py multicam/build/Desktop_Qt_6_9_0_MSVC2022_64bit-Release/scene/myface_undistort
```

### GS í•™ìŠµ ì„±ê³µ!
```python
<KIST_llff>
|---images
|   |---<image 0>
|   |---<image 1>
|   |---...
|---sparse
|    |---0
|        |---cameras.bin
|        |---images.bin
|        |---points3D.bin
|---poses_bounds.npy   
```

------------------------------------------------------------------------------------------------------------------
# To do list
- feature extractionì„ denseí•˜ê²Œ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê³ , feature matchingì„ ìˆ˜í–‰í•˜ê³ , PnP ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ êµ¬í•œ ì¹´ë©”ë¼ í¬ì¦ˆì™€ í•¨ê»˜, ì•ì„œ êµ¬í•œ feature matchingìœ¼ë¡œ ì°¾ì•„ì§„ correspondencesê°€ 2ê°œ ì´ìƒì¼ ê²½ìš°ì—ë„ triangulationí•˜ì—¬ 3D í¬ì¸íŠ¸ë¥¼ reconstructí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì•¼í•˜ê³ , ì¶”ê°€ì ìœ¼ë¡œ ì¹´ë©”ë¼ í¬ì¦ˆ ë° 3D êµ¬ì¡°ê°„ì˜ ì •í•© ìµœì í™”ë¥¼ ìœ„í•œ Bundle Adjustment(BA)ë¥¼ ìˆ˜í–‰í•´ì•¼ë§Œ í•¨

## Triangulation with more than 3 correspondences (cv2.sfm)
### [MATLAB triangulateMultiview](https://www.mathworks.com/help/vision/ref/triangulatemultiview.html)
triangulateMultiview 3-D locations of world points matched across multiple images

worldPoints = triangulateMultiview(pointTracks,cameraPoses,intrinsics) returns the locations of 3-D world points that correspond to points matched across multiple images taken with calibrated cameras. **pointsTracks specifies an array of matched points**. cameraPoses and intrinsics specify camera pose information and intrinsics, respectively. The function does not account for lens distortion.
[worldPoints,reprojectionErrors] = triangulateMultiview(__) additionally returns the mean reprojection error for each 3-D world point using all input arguments in the priro syntax.
[worldPoints,reprojectionErrors,validIndex] = triangulateMultiview(__) additionally returns the indices of valid and invalid world points. Valid points are locaed in front of the cameras.

Load images in the workspace
**Load precomputed cameraparameters** --> Get camera intrinsics parameters
Compute features for the first images.
```python
I = im2gray(readimage(imds,1));;
**I = undistortImage(I,intrinsics);**
pointsPrev = detectSURFFeatures(I);;
[featuresPrev,pointsPrev] = extractFeatures(I,pointsPrev);;
```

Load camera poses.
Creae an imageviewset object
vSet = imageviewset;
vSet = addView(vSet,1,absPoses(1),Points=pointsPrev);

Compute features and matches for the rest of the images.
for i = 2:numel(imds.Files)
  I = im2gray(readimage(imds,i));
  I = undistortImage(I,intrinsics);
  points = detectSURFFeatures(I);
  [features,points] = extractFeatures(I,points);
  vSet = addView(vSet,i,absPoses(i),Points=points);
  pairsIdx = matchFeatures(featuresPrev,features,MatchThreshold=5);
  vSet = addConnection(vSet,i-1,i,Matches=pairsIdx);
  featuresPrev = features;
end

Find point tacks.
tracks = findTracks(vSet);
get camera poses.
caneraPoses = poses(vSet);
Find 3-D world points.
[xyzPoints,errors] = triangulateMultiview(tracks,cameraPoses,intrinsics);
z = xyzPoints(:,3);
idx = errors < 5 & z > 0 & z < 20;
pcshow(xyzPoints(idx, :),AxesVisibility="on",VerticalAxis="y",VerticalAxisDir="down",MarkerSize=30);
hold on
plotCamera(cameraPoses, Size=0.2);
hold off

### [MATLAB matchFeatures](https://www.mathworks.com/help/vision/ref/matchfeatures.html?searchHighlight=matchFeatures&s_tid=srchtitle_support_results_1_matchFeatures)
matchFeatures Find matching features

indexPairs = matchFeatures(features1,features2) returns indices of the matching features in the two input feature sets. The input feature must be either binaryFeatures objects or matrices.
[indexPairs,matchmetric] = matchFeatures(features1,features2) also returns the distance between the matching features, indexed by indexPairs.
[indexPairs,matchmetric] = matchFeatures(features1,features2,Name=Value) specifies options using one or more name-value arguments in addition to any combination of arguments from previous syntaxes. For example, matchFeatures(__,Method="Exhaustive") sets the matching method to Exhaustive.

Find Corresponding Interest Points Between Pair of Images
Find corresponding interest points between a pair of images using local neighborhoods and the Harris algorithm.
Read the stereo images.
I1 = im2gray(imread("viprectification_deskLeft.png"));
I2 = im2gray(imread("viprectification_deskRight.png"));

Find the corners.
points1 = detectHarrisFeatures(I1);
points2 = detectHarrisFeatures(I2);

Extract the neighborhood features.
[features1,valid_points1] = extractFeatures(I1,points1);
[features2,valid_points2] = extractFeatures(I2,points2);

Match the features.
indexPairs = matchFeatures(features1,features2);

Retrieve the locations of the corresponding points for each image.
matchedPoints1 = valid_points1(indexPairs(:,1),:);
matchedPoints2 = valid_points2(indexPairs(:,2),:);

Visualize the corresponding points. You can see the effect of translation between the two images despite several eeroneous matches.
figure;
showMatchedFeatures(I1,I2,matchedPoints1,matchedPoints2);

Find Corresponding Points Using SURF Features
Read the two images.
I1 = imread("cameraman.tif");
I2 = imresize(imrotate(I1,-20),1.2);

Find the SURF features.
points1 = detectSURFFeatures(I1);
points2 = detectSURFFeatures(I2);

Extract the features.
[f1,vpts1] = extractFeatures(I1,points1);
[f2,vpts2] = extractFeatures(I2,points2);

Retrieve the locations of matched points.
indexPairs = matchFeatures(f1,f2);
matchedPoints1 = vpts1(indexPairs(:,1));
matchedPoints2 = vpts2(indexPairs(:,2));

Display the matching points. The data still includes several outliers, but you can see the effects of rotation and scaling on the display of matched features.

figure; showMatchedFeatures(I1,I2,matchedPoints1,matchedPoints2);
legend("matched points 1","matched points 2");

### [MATLAB findTracks](https://www.mathworks.com/help/vision/ref/imageviewset.findtracks.html)

findTracks -> Find matched points across multiple views

tracks = findTracks(vSet) finds and returns point tracks across multiple views in the view set, vSet. Each track contains 2-D projections of the same 3-D world point.
tracks = findTracks(vSet, viewIds) finds point tracks across a subset of views specified by viewIds.
tracks = findTracks(__, 'MinTrackLength', trackLength) specifies the minimum length of the tracks.

Compute features for the first image.
I = im2gray(readimage(images,1));
pointsPrev = detectSURFFeatures(I);
[featuresPrev,pointsPrev] = extractFeatures(I,pointsPrev)

Create an image view set and add one view to the set.
vSet = imageviewset;
Vset = addView(vSet,1,'Features',featuresPrev,'Points',pointsPrev);

Compute features and matches for the rest of the images.
for i = 2:numel(images.Files)
  I = im2gray(readimage(images,i));
  points = detectSURFFeatures(I);
  [features, points] = extractFeatures(I,points);
  vSet = addView(vSet,i,'Features',features,'Points',points);
  pairsIdx = matchFeatures(featuresPrev,features);
  vSet = addConnection(vSet,i-1,i,'Matches',pairsIdx);
  featuresPrev = features;
end

Find point tracks across views in the image view set.
tracks = findTracks(vSet);

vSet -- Image view set 
Image view set, specified as an imageviewset object.

### [MATLAB imageviewset](https://www.mathworks.com/help/vision/ref/imageviewset.html)
Manage data for structure-from-motion, visual odometry, and visual SLAM

The imageviewset object manages view attributes and pairwise connections between views of data used in structure-from-motion, visual odometry, and simultaneous localization and mapping (SLAM) data. View attributes can be feature descriptors, feature points, or absolute camera poses. Pairwise connections between views can be point matches, relative camera poses, or an information matrix. You can also use this object to find point tracks used by triangulateMultiview and bundleAdjustment functions.

vSet = imageviewset() returns an imageviewset object. You can add views and connections using the addView and addConnection object functions.

### [MATLAB addView](https://www.mathworks.com/help/vision/ref/imageviewset.addview.html)
addView add view to view set

vSet = addView(vSet,viewID) adds the view specified by viewID to the view set, vSet.
vSet = addView(vSet,viewID,absPose) specifies the absolute pose of the view.
vSet = addView(__,Name,Value) specifies options using one or more name-value arguments in addition to any of the input argument combination in previous syntaxes.
vSet = addView(vSet,viewTable) adds one or more views in the table specified by viewTable.

Create an empty image view set.
vSet = imageviewset;

Detect interest points in the image.
points = detectSURFFeatures(im2gray(I))

Add the interest points as a view to the image view set.
vSet = addView(vSet,1,'Points',points);

### [MATLAB addConnection](https://www.mathworks.com/help/vision/ref/imageviewset.addconnection.html)
addConnection Add connection between views in view set

vSet = addConnection(vSet,viewId1,viewId2) adds a connection between views viewId1 and viewId2 to the view set, vSet.
vSet = addConnection(vSet,viewId1,viewId2,relPose) specifies the relative pose of viewId2 with respect to viewId1.
vSet = addConnection(vSet,viewId1,viewId2,relPose,infoMat) specifies the information matrix associated with the connection.
vSet = addConnection(__,"Matches",featureMatches) specifies the indices of matched points between two views in addition to any of the input argument combinations in previous syntaxes.

Create an empty image view set.
vSet = imageviewset;

Read two images into the workspace
I1 = im2gray(imread(fullfile(imageDir,'image1.jpg')));
I2 = im2gray(imread(fullfile(imageDir,'image2.jpg')));

Detect interest points in each image.
points1 = detectSURFFeatures(I1);
points2 = detectSURFFeatures(I2);

Extract feature descriptors from the interest points.
[features1,validPoints1] = extractFeatures(I1,points1);
[features2,validPoints2] = extractFeatures(I2,points2);

Add teh features and points for the two images to the image view set.
vSet = addView(vSet,1,'Features',features1,'Points',validPoints1);
vSet = addView(vSet,2,'Features',features2,'Points',validPoints2);

Match teh features between the two images.
indexPairs = matchFeatures(features1,features2);

Store the matched features as a connection in the image view set.
vSet = addConnection(vSet,1,2,'Matches',indexPairs);

### [MATLAB extractFeatures](https://www.mathworks.com/help/vision/ref/extractfeatures.html)
extractFeatures Extract interest point descriptors

[features,validPoints] = extractFeatures(I,points) returns **extracted feature vectors, also known as descriptor**, and their corresponding locations, from a binary or intensity image. The function derives the descriptors from pixels surrounding an interest point. The pixels represent and match features specified by a single-point location. Each single-point specifies the center location of a neighborhood. The method you use for descriptor extraction depends on the class of the input points.
[features,validPoints] = extractFeatures(I,points,Name=Value) specifies options using one or more name-value arguments in addition to any combination of arguments from previous syntaxes. For example, extractFeatures(I,points,Method="Block") sets teh method to extract descriptor to Block.

Extract Corner Features from an Image
Read the image
I = imread("cameraman.tif")
 
Find and extract corner features from the image.
corners = detectHarrisFeatures(I);
[features,valid_corners] = extractFeatures(I,corners);

Extract SURF Features from an Image
Read image
I = imread("camearman.tif")

Find and extract features from the input image.
points = detectSURFFeatures(I);
[features,valid_points] = extractFeatures(I,points);


### [MATLAB bundleAdjustment](https://www.mathworks.com/help/vision/ref/bundleadjustment.html)
bundleAdjustment **Adjust collection of 3-D points and camera poses**

[xyzRefinedPoints,refinedPoses] = bundleAdjustment(xyzPoints,pointTracks,cameraPoses,intrinsics) refines 3-D points and camera poses to minimize reprojection errors. The refinement procedure is a variant of the Levenberg-Marquardt algorithm. The function **uses the same global reference coordinate system to return both the 3-D points and camera poses.**
[wpSetRefined,vSetRefined,pointIndex] = bundleAdjustment(wpSet,vSet,viewIds,intrinsics) refines 3-D points from the world point set, wpSet, and refines camera poses from the image view set, vSet. viewIds specify the camera poses in vSet to refine.
[__,reprojectionErrors] = bundleAdjustment(__) returns the mean reprojection error for each 3-D world point, in addition to the arguments from the previous syntax.
[__] = bundleAdjustment(__,Name=Value) specifies options using one or more name-value arguments in addition to any combination of arguments from previous syntaxes. For example, **MaxIterations=50 sets the number of iterations to 50**. Unspecified arguments have default values.

Load data for initialization.
data = load("globalBA.mat")
Refine the camera poses and points.
[xyzRefinedPoints,refinedPoses] = ...
  bundleAdjustment(data.xyzPoints,data.pointTracks,data.cameraPoses,data.intrinsics);
Display the 3-D points and camera poses before and after refinement.
pcshowpair(pointCloud(data.xyzPoints), pointCloud(xyzRefinedPoints), ...
    AxesVisibility="on", VerticalAxis="y", VerticalAxisDir="down", MarkerSize=40);
hold on
plotCamera(data.cameraPoses, Size=0.1, Color="m");
plotCamera(refinedPoses, Size=0.1, Color="g");
legend("Before refinement", "After refinement", color="w");

Input Arguments
xyzPoints: Unrefined 3-D points, specified as an M-by-3 matrix of [x y z] locations.
pointTracks: Matching points across multiple images, specified as an N-element array of pointTrack objects. Each element contains two or more matching points across multiple images.
cameraPoses: Camera pose information, specified as a two-column table with columns ViewId and AbsolutePose. The view IDs relate to the IDs of the objects in the pointTracks argument. You can use the poses object function to obtain cameraPoses table.
intrinsics: Camera intrinsics, specified as a cameraIntrinsics object or an N-element array of cameraIntrinsics objects. N is the number of camera poses or the number of IDs in viewIDs. Use a single cameraIntrinsics object when images are captured using the same camera. Use a vector cameraIntrinsics objects when images are captured by different cameras.
wpSet: 3-D world points, specified as a worldpointset object.
vSet: Camera poses, specified as an imageviewset object.
viewIDs: View identifiers, specified as an N-element array. The viewIDs represent which camera poses to refine specifying their related views in imageviewset.

Name-Value Arguments
MaxIterations: Maximum number of iterations before the Levenberg-Marquardt algorithm stops, specified as a positive integer.
AbsoluteTolerance: Absolute termination tolerance of the mean squared reprojection error in pixels, specified as positive scalar.
RelativeTolerance: Relative termination tolerance of the reduction in reprojection error between iterations, specified as positive scalar.
PointsUndistorted: Flag to indicate lens distortion, specified as false or true. **When you set PointsUndistorted to false, the 2-D points in pointTracks or in vSet must be from images with lens distortion.** **To use undistorted points, first use the undistortImage function to remove distortions from the images, then set PointsUndistorted.**
FixedViewIDs: View IDs for fixed camera pose, specified as a vector of nonnegative integers. Each ID corresponds to the ViewId of a fixed camera pose in cameraPoses. An empty value for FixedViewIDs means that all camera poses are optimized.

Output Arguments
reprojectionErrors: Reprojection errors, returned as an M-element vector. The function projects each world point back into each camera. Then, in each image, the function calculates the reprojection error as the distance between the detected and the reprojected point. The reprojectionErrors vector contains the average reprojection error for each world point.

  <p align="center">
    <img src="https://github.com/user-attachments/assets/4b6664c1-f424-465f-b27f-0bc74c9ca0ed" width="600"/>
  </p>
  <p align="center"><em>Reprojection Errors</em></p>




### [OpenCV triangulatePoints í•¨ìˆ˜](https://docs.opencv.org/4.x/d0/dbd/group__triangulation.html)
- Triangulates the 3d position of 2d correspondences between several images. Reference: Internally it uses DLT method [119] 12.2 pag.312
- [119] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.
- cv2.sfm.triangulatePoints(InputArrrayOfArrays points2d, InputArrayOfArrays projection_matrices, OutputArray points3d)
  - ì œê³µëœ ì½”ë“œì˜ í•µì‹¬ì€ 2ê°œ ë·°(2-view)ì˜ ê²½ìš°ì™€ 3ê°œ ì´ìƒì˜ ë·°(N-view)ì˜ ê²½ìš°ë¥¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ë©°, ë‘ ê²½ìš° ëª¨ë‘ DLT(Direct Linear Transformation) ì›ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3D ì¢Œí‘œë¥¼ ê³„ì‚°í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.
  - ë·°ì˜ ê°œìˆ˜(nviews)ê°€ ì •í™•íˆ 2ê°œì¼ ê²½ìš°, triangulateDLT í•¨ìˆ˜ë¥¼ ê° ì  ìŒì— ëŒ€í•´ í˜¸ì¶œí•©ë‹ˆë‹¤.
    - 2-ë·°ì˜ ê²½ìš°(triangulateDLT): ê° ëŒ€ì‘ì  ìŒìœ¼ë¡œë¶€í„° 4ê°œì˜ ì„ í˜• ë°©ì •ì‹ì„ ë§Œë“¤ì–´ AX=0 ì‹œìŠ¤í…œì„ êµ¬ì„±í•˜ê³ , SVDë¥¼ ì´ìš©í•´ Xë¥¼ ì§ì ‘ í’‰ë‹ˆë‹¤. ì´ëŠ” ì±… 12.2ì ˆì— ê¸°ìˆ ëœ í‘œì¤€ DLT ë°©ë²•ê³¼ ì •í™•íˆ ì¼ì¹˜í•©ë‹ˆë‹¤. 
  - ë·°ì˜ ê°œìˆ˜ê°€ 2ê°œë³´ë‹¤ ë§ì„ ê²½ìš°, triangulateNViews í•¨ìˆ˜ë¥¼ ê° ì  íŠ¸ë™ì— ëŒ€í•´ í˜¸ì¶œí•©ë‹ˆë‹¤.
    - ë‹¤ì¤‘ ë·°ì˜ ê²½ìš°(triangulateNViews): 3D ì  Xì™€ ê° ë·°ì˜ ê¹Šì´ ìŠ¤ì¼€ì¼ Î»ië¥¼ ëª¨ë‘ ë¯¸ì§€ìˆ˜ë¡œ ë‘ëŠ” ë” í° ì„ í˜• ì‹œìŠ¤í…œì„ êµ¬ì„±í•©ë‹ˆë‹¤. SVDë¥¼ í†µí•´ ì´ ì‹œìŠ¤í…œì„ í‘¼ ë’¤, 3D ì ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ìµœì¢… ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª¨ë“  ë·°ì˜ ì •ë³´ë¥¼ ë™ì‹œì— í™œìš©í•˜ì—¬ ë” ì•ˆì •ì ì´ê³  ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - ë‹¤ì¤‘ ë·°(3ê°œ ì´ìƒ)ì— ëŒ€í•´ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì…ë ¥ì€ `points2d`ì™€ `projection_matrices` ë‘ ê°€ì§€ì…ë‹ˆë‹¤.
    - points2d: 2D ëŒ€ì‘ì  ë°ì´í„°
      - ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ì—¬ëŸ¬ ë·°ì— ê±¸ì³ ì¶”ì ëœ 2D ì ë“¤ì˜ ì§‘í•©ì…ë‹ˆë‹¤.
      - ì˜ˆë¥¼ ë“¤ì–´, 3ê°œì˜ ë·°ì—ì„œ 100ê°œì˜ ì ì„ ì‚¼ê°ì¸¡ëŸ‰í•œë‹¤ë©´:
        - points2dëŠ” 3ê°œì˜ Mat ê°ì²´ë¥¼ ë‹´ê³  ìˆëŠ” vectorê°€ ë©ë‹ˆë‹¤.
        - ê° Matì€ 2x100 í¬ê¸°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
        - points2d[0].col(5)ëŠ” ì²« ë²ˆì§¸ ë·°ì˜ 6ë²ˆì§¸ ì ì˜ (x,y) ì¢Œí‘œì…ë‹ˆë‹¤.
        - points2d[1].col(5)ëŠ” ë‘ ë²ˆì§¸ ë·°ì˜ 6ë²ˆì§¸ ì ì˜ (x,y) ì¢Œí‘œì…ë‹ˆë‹¤.
        - points2d[2].col(5)ëŠ” ì„¸ ë²ˆì§¸ ë·°ì˜ 6ë²ˆì§¸ ì ì˜ (x,y) ì¢Œí‘œì…ë‹ˆë‹¤.
        - ì´ ì„¸ ì  points2d[0].col(5), points2d[1].col(5), points2d[2].col(5)ëŠ” ëª¨ë‘ ë™ì¼í•œ 3D ê³µê°„ìƒì˜ í•œ ì ì—ì„œ íˆ¬ì˜ëœ ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    - `projection_matrices`: íˆ¬ì˜ í–‰ë ¬ ë°ì´í„°
      - ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ê° ë·°ì— í•´ë‹¹í•˜ëŠ” ì¹´ë©”ë¼ì˜ íˆ¬ì˜ í–‰ë ¬(ì¹´ë©”ë¼ í–‰ë ¬) ì§‘í•©ì…ë‹ˆë‹¤.
      - ë²¡í„°ì˜ í¬ê¸°ëŠ” ì „ì²´ ë·°ì˜ ê°œìˆ˜ mìœ¼ë¡œ, points2d ë²¡í„°ì˜ í¬ê¸°ì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
      - ë²¡í„°ì˜ ê° ìš”ì†Œì¸ cv::Mat ê°ì²´ëŠ” 3x4 í¬ê¸°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
      - ëª¨ë“  íˆ¬ì˜ í–‰ë ¬ P_iëŠ” ë°˜ë“œì‹œ ë™ì¼í•œ ì›”ë“œ ì¢Œí‘œê³„(world coordinate system)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í‘œí˜„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
      - ì´ í–‰ë ¬ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì „ ë‹¨ê³„ì—ì„œ ê³„ì‚°ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Fundamental Matrix ë˜ëŠ” Trifocal Tensorë¥¼ ê³„ì‚°í•œ ë’¤ ì´ë¡œë¶€í„° ì¹´ë©”ë¼ í–‰ë ¬ì„ ë³µì›í•˜ê±°ë‚˜(projective reconstruction), ë²ˆë“¤ ì¡°ì •(bundle adjustment)ì„ í†µí•´ ì–»ì–´ì§„ ê²°ê³¼ë¬¼ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      - ê°ê¸° ë‹¤ë¥¸ ì‹œì ì— ë…ë¦½ì ìœ¼ë¡œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ëœ ì¹´ë©”ë¼ í–‰ë ¬ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´, ê° ì¹´ë©”ë¼ê°€ ìì‹ ë§Œì˜ ì›”ë“œ ì¢Œí‘œê³„ë¥¼ ê°€ì§€ë¯€ë¡œ ì˜¬ë°”ë¥¸ ì‚¼ê°ì¸¡ëŸ‰ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.

- ì‚¬ìš©ìê»˜ì„œ MASt3Rë¡œ êµ¬í˜„í•œ pairwise matchingì€ Correspondence Trackingì˜ í›Œë¥­í•œ ì²« ë‹¨ê³„ì…ë‹ˆë‹¤. ì´ë¥¼ ë‹¤ì¤‘ ë·° íŠ¸ë˜í‚¹ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ì „ì²´ì ì¸ íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  - Pairwise Matching: ì¸ì ‘í•œ ì´ë¯¸ì§€ ìŒë“¤ì— ëŒ€í•´ MASt3Rë¥¼ ì‹¤í–‰í•˜ì—¬ matches(i, i+1)ë¥¼ êµ¬í•©ë‹ˆë‹¤.
  - Track Chaining: ì´ ìŒë³„ ë§¤ì¹­ë“¤ì„ ì—°ê²°í•˜ì—¬ ê¸´ í›„ë³´ íŠ¸ë™ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
  - Robust Verification: RANSACê³¼ Trifocal Tensorë¥¼ ì´ìš©í•˜ì—¬ í›„ë³´ íŠ¸ë™ë“¤ ì¤‘ ê¸°í•˜í•™ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ inlier íŠ¸ë™ë“¤ë§Œ ì„ ë³„í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì±…ì˜ Algorithm 16.4ì— ì˜ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
  - ì´ë ‡ê²Œ ìµœì¢…ì ìœ¼ë¡œ ì–»ì–´ì§„ inlier ëŒ€ì‘ì  íŠ¸ë™ë“¤ì´ ë°”ë¡œ cv::sfm::triangulatePoints í•¨ìˆ˜ì˜ points2d ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ê³ í’ˆì§ˆ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì™€ í•¨ê»˜, RANSAC ê³¼ì •ì—ì„œ ì–»ì€ ì¹´ë©”ë¼ í–‰ë ¬ë“¤(Trifocal Tensorë¡œë¶€í„° ë³µì›)ì„ projection_matrices ì…ë ¥ìœ¼ë¡œ ì£¼ë©´, ë§¤ìš° ì •í™•í•˜ê³  ì•ˆì •ì ì¸ ë‹¤ì¤‘ ë·° ì‚¼ê°ì¸¡ëŸ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Correspondence Trackingì„ ìœ„í•œ ë‹¨ê³„ë³„ ì ˆì°¨
#### 1ë‹¨ê³„: ìˆœì°¨ì  ìŒë³„ ë§¤ì¹­ (Sequential Pairwise Matching)
- ì‚¬ìš©ìê»˜ì„œ ì´ë¯¸ êµ¬í˜„í•˜ì‹  ì½”ë“œê°€ ì´ ë‹¨ê³„ì— í•´ë‹¹í•©ë‹ˆë‹¤. mê°œì˜ ë·°(ì´ë¯¸ì§€)ê°€ ìˆë‹¤ë©´, ì¸ì ‘í•œ ëª¨ë“  ì´ë¯¸ì§€ ìŒì— ëŒ€í•´ MASt3R ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
  - view 1ê³¼ view 2 ì‚¬ì´ì˜ ë§¤ì¹­ matches(1,2)ë¥¼ êµ¬í•©ë‹ˆë‹¤.
  - view 2ì™€ view 3 ì‚¬ì´ì˜ ë§¤ì¹­ matches(2,3)ë¥¼ êµ¬í•©ë‹ˆë‹¤.
  - view 3ê³¼ view 4 ì‚¬ì´ì˜ ë§¤ì¹­ matches(3,4)ë¥¼ êµ¬í•©ë‹ˆë‹¤.
  - ...
  - view m-1ê³¼ view m ì‚¬ì´ì˜ ë§¤ì¹­ matches(m-1,m)ì„ êµ¬í•©ë‹ˆë‹¤.
  - ì´ ë‹¨ê³„ì˜ ê²°ê³¼ë¬¼ì€ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ 2-view ëŒ€ì‘ì  ëª©ë¡ì…ë‹ˆë‹¤.

#### 2ë‹¨ê³„: ë§¤ì¹­ ì—°ê²°ì„ í†µí•œ í›„ë³´ íŠ¸ë™ ìƒì„± (Chaining Matches to Form Putative Tracks)
- ì´ì œ ê° ìŒë³„ ë§¤ì¹­ ê²°ê³¼ë¥¼ ì—°ê²°í•˜ì—¬ ì—¬ëŸ¬ í”„ë ˆì„ì— ê±¸ì¹œ íŠ¸ë™ì„ ë§Œë“­ë‹ˆë‹¤. ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  - matches(1,2)ì—ì„œ í•œ ëŒ€ì‘ì  ìŒ (p1, p2)ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. p1ì€ view 1ì˜ ì , p2ëŠ” view 2ì˜ ì ì…ë‹ˆë‹¤. ì´ê²ƒì´ íŠ¸ë™ì˜ ì‹œì‘ì…ë‹ˆë‹¤.
  - matches(2,3) ëª©ë¡ì—ì„œ p2ì™€ ì¼ì¹˜í•˜ê±°ë‚˜ ë§¤ìš° ê°€ê¹Œìš´ ì ì„ ì°¾ìŠµë‹ˆë‹¤. ë§Œì•½ (p2, p3)ë¼ëŠ” ë§¤ì¹­ì„ ì°¾ì•˜ë‹¤ë©´, íŠ¸ë™ì„ (p1, p2, p3)ë¡œ í™•ì¥í•©ë‹ˆë‹¤.
  - ë‹¤ì‹œ matches(3,4) ëª©ë¡ì—ì„œ p3ì— í•´ë‹¹í•˜ëŠ” ì  p4ë¥¼ ì°¾ì•„ íŠ¸ë™ì„ (p1, p2, p3, p4)ë¡œ í™•ì¥í•©ë‹ˆë‹¤.
  - ì´ ê³¼ì •ì„ ë” ì´ìƒ ì—°ê²°í•  ë§¤ì¹­ì´ ì—†ì„ ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
  - ì´ ê³¼ì •ì„ ëª¨ë“  matches(1,2)ì˜ ëŒ€ì‘ì ì— ëŒ€í•´ ìˆ˜í–‰í•˜ë©´, ì—¬ëŸ¬ ê°œì˜ ë‹¤ì–‘í•œ ê¸¸ì´ë¥¼ ê°€ì§„ í›„ë³´ íŠ¸ë™(putative tracks) ì§‘í•©ì´ ìƒì„±ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ê° ë‹¨ê³„ì—ì„œ ì‘ì€ ì˜¤ì°¨ê°€ ëˆ„ì ë˜ì–´ 'í‘œë¥˜(drift)'ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë©°, ì˜ëª»ëœ ë§¤ì¹­ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê¸°í•˜í•™ì  ê²€ì¦ì´ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.

#### 3ë‹¨ê³„: ë‹¤ì¤‘ ë·° ê¸°í•˜í•™ì„ ì´ìš©í•œ ê°•ì¸í•œ ê²€ì¦ (Robust Geometric Verification)
- ì´ ë‹¨ê³„ê°€ ë°”ë¡œ ì œê³µí•´ì£¼ì‹  Hartley & Zisserman ì±…ì˜ ì´ë¡ ì´ ê²°ì •ì ì¸ ì—­í• ì„ í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì—°ê²°ë§Œ ëœ íŠ¸ë™ì€ ê¸°í•˜í•™ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ë‹¤ëŠ” ë³´ì¥ì´ ì—†ìŠµë‹ˆë‹¤. ì±…ì˜ Chapter 15ì™€ 16ì—ì„œ ì„¤ëª…í•˜ëŠ” **ì‚¼ì¤‘ì´ˆì  í…ì„œ(Trifocal Tensor)**ë¥¼ ì‚¬ìš©í•˜ë©´ 3ê°œ ë·°ì— ê±¸ì¹œ ëŒ€ì‘ì ì˜ ìœ íš¨ì„±ì„ ë§¤ìš° ê°•ë ¥í•˜ê²Œ ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Trifocal Tensorë¥¼ ì´ìš©í•œ ê²€ì¦:
  - Trifocal Tensorì˜ í•µì‹¬ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” ì „ì†¡(Transfer) ì…ë‹ˆë‹¤ (ì±…ì˜ Section 15.3 ì°¸ì¡°).
  - ë‘ ë·°(ì˜ˆ: view 1ê³¼ view 2)ì—ì„œ ëŒ€ì‘ì  (p1, p2)ê°€ ì£¼ì–´ì§€ë©´, Trifocal TensorëŠ” ì„¸ ë²ˆì§¸ ë·°ì—ì„œ í•´ë‹¹ ì  p3ê°€ ë‚˜íƒ€ë‚˜ì•¼ í•  ì •í™•í•œ ìœ„ì¹˜ p3_predictedë¥¼ ê³„ì‚°í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ì´ëŠ” 2ê°œ ë·°ì—ì„œ ë‹¨ì§€ 'epipolar line ìœ„ì— ìˆì–´ì•¼ í•œë‹¤'ëŠ” ì œì•½ë³´ë‹¤ í›¨ì”¬ ê°•ë ¥í•©ë‹ˆë‹¤. ì´ ì›ë¦¬ë¥¼ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì´ ì˜ëª»ëœ íŠ¸ë™ì„ ê±¸ëŸ¬ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- RANSAC í”„ë ˆì„ì›Œí¬ ì ìš© (Algorithm 16.4 ì°¸ì¡°):
  - 2ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ìˆ˜ë§ì€ í›„ë³´ íŠ¸ë™ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ 6ê°œì˜ íŠ¸ë™ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. (Trifocal TensorëŠ” ìµœì†Œ 6ê°œì˜ ì  ëŒ€ì‘ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ).
  - ì´ 6ê°œì˜ íŠ¸ë™ì„ ì´ìš©í•´ Trifocal Tensor Të¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
  - ê³„ì‚°ëœ Të¥¼ ì‚¬ìš©í•˜ì—¬ ë‚˜ë¨¸ì§€ ëª¨ë“  íŠ¸ë™ë“¤ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤. ê° íŠ¸ë™ (p1, p2, p3)ì— ëŒ€í•´, p1ê³¼ p2ë¥¼ ì´ìš©í•´ ì„¸ ë²ˆì§¸ ë·°ì˜ ì  ìœ„ì¹˜ p3_predictedë¥¼ ì „ì†¡(transfer)í•©ë‹ˆë‹¤.
  ì‹¤ì œ ì¸¡ì •ëœ p3ì™€ ì˜ˆì¸¡ëœ p3_predicted ì‚¬ì´ì˜ ê±°ë¦¬(ì¬íˆ¬ì˜ ì˜¤ì°¨)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. d(p3, p3_predicted).
  - ì´ ê±°ë¦¬ê°€ ë¯¸ë¦¬ ì •í•´ë‘” ì„ê³„ê°’ të³´ë‹¤ ì‘ìœ¼ë©´, í•´ë‹¹ íŠ¸ë™ì„ **ì •ìƒê°’(inlier)**ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
  - ê°€ì¥ ë§ì€ ìˆ˜ì˜ inlierë¥¼ í™•ë³´í•˜ëŠ” Trifocal Tensor Të¥¼ ìµœì¢… ëª¨ë¸ë¡œ ì„ íƒí•©ë‹ˆë‹¤.
- ìµœì¢… ëŒ€ì‘ì  íŠ¸ë™ í™•ë³´:
  - RANSAC ê³¼ì •ì—ì„œ ê°€ì¥ ë§ì€ ì§€ì§€ë¥¼ ë°›ì€ Trifocal Tensor ëª¨ë¸ê³¼ ì¼ê´€ì„±ì„ ë³´ì¸ inlier íŠ¸ë™ë“¤ì´ ë°”ë¡œ ìš°ë¦¬ê°€ ì°¾ë˜, ê¸°í•˜í•™ì ìœ¼ë¡œ ê²€ì¦ëœ ìµœì¢… ëŒ€ì‘ì  íŠ¸ë™ì´ ë©ë‹ˆë‹¤.



### cv2.sfmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ python 3.11 ê°€ìƒí™˜ê²½ ìƒˆë¡œ êµ¬ì¶• (mast3rì˜ faiss-gpu ì‚¬ìš©ì„ ìœ„í•´ CUDA 12.1ë¡œ ì„¤ì¹˜)
- [OPENCV_EXTRA_MODULESì— sfmì´ í¬í•¨ë˜ì–´ ìˆìŒ](https://github.com/opencv/opencv_contrib/blob/master/modules/sfm/src/triangulation.cpp)
- `cv2.sfm` ëª¨ë“ˆì€ OpenCVì˜ contrib ëª¨ë“ˆ ì¤‘ í•˜ë‚˜ì´ë©°, ê¸°ë³¸ OpenCV ì„¤ì¹˜ì—ëŠ” í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. cv2.sfmì„ ì‚¬ìš©í•˜ë ¤ë©´ OpenCVë¥¼ ì†ŒìŠ¤ì—ì„œ ì§ì ‘ ë¹Œë“œí•´ì•¼í•¨

```python
# mast3r ì„¤ì¹˜
conda create -n mast3r python=3.11 cmake=3.14.0 -y
conda activate mast3r 
pip install "numpy<2.0"
## cv2.sfm ì§ì ‘ ë¹Œë“œí•˜ì—¬ ì„¤ì¹˜
...
## mast34 ì„¤ì¹˜
conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia  # cuda 12.1ë¡œ ì¬ì„¤ì¹˜
cd submodules/mast3r
pip install -r requirements.txt
pip install -r dust3r/requirements.txt
pip install -r dust3r/requirements_optional.txt
## opencv ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê±°
pip uninstall -y opencv-python opencv-python-headless

## compile and install ASMK
pip install cython

cd submodules/asmk/cython/
cythonize *.pyx
cd ..
conda install -c conda-forge faiss-gpu # faiss-gpu, containing both CPU and GPU indices, is available on Linux (x86-64 only) for CUDA 11.4 and 12.1 / For FRM, MASt3R pipeline internally uses Faiss library to store correspondences.
pip install .  # or python3 setup.py build_ext --inplace
cd ..


# DKM ì„¤ì¹˜
pip install submodules/DKM 
# DKM ì„¤ì¹˜ì‹œ ê°™ì´ ê¹”ë¦° opencv ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê±°
pip uninstall -y opencv-python opencv-python-headless
```

## Dense Matching Algorithm
- [DUSt3R](https://github.com/naver/dust3r)
  - DUSt3R ê¸°ë³¸ ëª¨ë¸ë¡œ pixelë³„ 3D pointë¥¼ ì˜ˆì¸¡í–ˆì—ˆìŠµë‹ˆë‹¤. ì´ ê´€ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ focal lengthë„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- [MASt3R](https://github.com/naver/mast3r)
  - [MASt3R and MASt3R-SfM Explanation: Image Matching and 3D Reconstruction Results](https://learnopencv.com/mast3r-sfm-grounding-image-matching-3d/?utm_source=chatgpt.com)
  - In MASt3R, a pixel i in image 1 and a pixel j in image 2 are considered as true match if they correspond to the same ground truth 3D point. i.e. Each local descriptor in a image matches at max only a single descriptor in the other image. The network is trained to learn such descriptors while penalizing non-matching feature descriptors using InfoNCE loss which is much more effective than a simple 3D regression loss, as used in DUSt3R.
  - MASt3R effectively handles extreme viewpoint differences upto 180 degrees-scenarios that can be sometimes ambiguous to humans. This remarkable performance is primarily attributed to MASt3R and DUSt3Râ€™s 3D scene understanding and image matching techniques.
  <p align="center">
    <img src="https://github.com/user-attachments/assets/58dfdbfd-2c49-4a7c-bd21-ff1b696b7f59" width="600"/>
  </p>
  <p align="center"><em>MASt3R performance on Hard Cases</em></p>
- **Key Takeaways**
  - DUSt3R and MASt3R have excellent 3D scene understanding and performs in the wild zero shot. From the predicted 3D geometry focal length can be recovered making these models as a standalone and go to methods for 3D scene reconstruction and pose estimation. **Their success lies in firmly rooting image matching and finding correspondences as 3D in nature.** MASt3R predicts 3D correspondences, even in regions where there arenâ€™t much camera motion or for neatly opposing view of the scene.
  - DUSt3Rê³¼ MASt3Rì˜ ì„±ê³µì€ ì´ë¯¸ì§€ ë§¤ì¹­ê³¼ ëŒ€ì‘ì  ì°¾ê¸°ë¥¼ ë³¸ì§ˆì ìœ¼ë¡œ 3D ë¬¸ì œë¡œ ë‹¨ë‹¨íˆ ì •ë¦½í•œ ë°ì— ìˆìŠµë‹ˆë‹¤. 
  - MASt3RëŠ” ì¹´ë©”ë¼ ì›€ì§ì„ì´ ê±°ì˜ ì—†ëŠ” ì˜ì—­ì´ë‚˜ ì¥ë©´ì˜ ì •ë°˜ëŒ€ ì‹œì ì—ì„œë„ 3D ëŒ€ì‘ì ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- [DKM](https://github.com/Parskatt/DKM)

### ê¸°ì¡´ Image Matchingì˜ ë¬¸ì œì  
ì „í†µì ì¸ ì´ë¯¸ì§€ ë§¤ì¹­ ê¸°ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ ì„¸ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:
- (1) í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
- (2) ê° í‚¤í¬ì¸íŠ¸ì˜ ì£¼ë³€ ì˜ì—­ìœ¼ë¡œë¶€í„° ì§€ì—­ì ìœ¼ë¡œ ë¶ˆë³€í•˜ëŠ” descriptorë¥¼ ì¶”ì¶œ
- (3) Feaeture Spaceì—ì„œ descriptor ê°„ ê±°ë¦¬ ë¹„êµë¥¼ í†µí•´ í‚¤í¬ì¸íŠ¸ ë§¤ì¹­ ìˆ˜í–‰

ì´ëŸ¬í•œ ë°©ì‹ì€ ì¡°ëª… ë³€í™”ë‚˜ ì‹œì  ë³€í™”ì— ë¹„êµì  ê°•ì¸í•˜ë©°, ì ì€ ìˆ˜ì˜ í‚¤í¬ì¸íŠ¸ë§Œìœ¼ë¡œë„ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ì˜ ë¹ ë¥¸ ë§¤ì¹­ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì „ì—­ì ì¸ ê¸°í•˜í•™ì  ë¬¸ë§¥(geometric context)ì„ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ë°˜ë³µ íŒ¨í„´ì´ë‚˜ ì €í…ìŠ¤ì²˜ ì˜ì—­ì—ì„œëŠ” ì˜¤ì°¨ê°€ ë°œìƒí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ Image Matchingì€ 2Dë¬¸ì œë¡œ ë‹¤ë¤„ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ë°˜ë©´ DUSt3Rì—ì„œëŠ” Transformerê¸°ë°˜ìœ¼ë¡œ ë‘ ì´ë¯¸ì§€ì˜ pixelê³¼ 3D pointmapì˜ correspondence ì˜ˆì¸¡ì„ í†µí•´ 3Dê³µê°„ìƒì—ì„œ Image Matching ë¬¸ì œë¥¼ í’€ì—ˆìŠµë‹ˆë‹¤.
**í•˜ì§€ë§Œ DUSt3Rì´ 3D Reconstruction ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œê¸°ì—, ì‹œì  ë³€í™”ì—” ê°•ì¸í•˜ì§€ë§Œ Image Matchingì—ì„  ìƒëŒ€ì ìœ¼ë¡œ ë¶€ì •í™•í•©ë‹ˆë‹¤. MASt3Rì—ì„œëŠ” DUSt3Rì„ í™œìš©í•˜ì—¬ Image Matchingì— íŠ¹í™”í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ë‹¤ë£¹ë‹ˆë‹¤.**

  <p align="center">
    <img src="https://github.com/user-attachments/assets/5ec51bfc-85d5-4bb6-b0ad-0ebffab5d600" width="600"/>
  </p>
  <p align="center"><em>MASt3R performance on extreme viewpoint differences</em></p>

ìœ„ ê·¸ë¦¼ì€ MASt3Rì—ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. 2ê°œ ì…ë ¥ ì´ë¯¸ì§€ì˜ ê³µí†µ ì˜ì—­ì´ ì•„ì£¼ ì ìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , 2Dë¬¸ì œê°€ ì•„ë‹ˆë¼ 3Dë¬¸ì œë¡œ í’€ì—ˆê¸° ë•Œë¬¸ì—, Image Matchingì´ ì˜ ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ì´ë¯¸ì§€ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. 


## ğŸ” ìš©ì–´ ì •ë¦¬: "Dense Reconstruction"
Dense reconstructionì€ ì¼ë°˜ì ìœ¼ë¡œ MVS (Multi-View Stereo) ë¥¼ í†µí•´ ì¥ë©´ì˜ í‘œë©´ì„ ì¡°ë°€í•œ 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë˜ëŠ” meshë¡œ ë³µì›í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
ì´ë•ŒëŠ” triangulated sparse pointsê°€ ì•„ë‹Œ, í”½ì…€ ë‹¨ìœ„ë¡œ ëŒ€ì‘ì ì„ ì°¾ê³  ê¹Šì´ ì •ë³´ë¥¼ ì¶”ì •í•˜ì—¬ ìˆ˜ì‹­ë§Œ~ìˆ˜ë°±ë§Œ ê°œì˜ 3D ì ì„ ìƒì„±í•©ë‹ˆë‹¤.
â†’ COLMAPì—ì„œë„ MVSë¥¼ ì‚¬ìš©í•œ patch_match_stereo, stereo_fusion ì´í›„ê°€ ì§„ì§œ dense reconstructionì…ë‹ˆë‹¤.

### í”í•œ í˜¼ë™: Dense feature ì‚¬ìš© = dense reconstruction?
ì–´ë–¤ ì—°êµ¬ë‚˜ êµ¬í˜„ì—ì„œëŠ” dense feature matcher (e.g., LoFTR)ë§Œ ì‚¬ìš©í•´ë„ ì´ë¥¼ "dense reconstruction"ì´ë¼ ë¶€ë¥´ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—„ë°€íˆ ë³´ë©´ ì´ê±´: dense correspondences ê¸°ë°˜ì˜ SfM ë˜ëŠ” dense SfM ì´ë¼ê³  ë¶€ë¥´ëŠ” ê²Œ ë” ì •í™•í•©ë‹ˆë‹¤.

## ğŸ” ìš©ì–´ ì •ë¦¬: Map-free relocalization
Map-free relocalizationì€ ê¸°ì¡´ì˜ ì§€ë„ ì—†ì´ ì£¼ì–´ì§„ ì´ë¯¸ì§€ì—ì„œ ì¹´ë©”ë¼ì˜ ìœ„ì¹˜ ë° ë°©í–¥(6DoF pose) ì„ ì¶”ì •í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.
ì¦‰, SLAMì´ë‚˜ SfMìœ¼ë¡œ ë¯¸ë¦¬ êµ¬ì¶•ëœ 3D map ì—†ì´, ì˜¤ì§ ì´ë¯¸ì§€ ìŒ(ë˜ëŠ” ë‹¤ì¤‘ ì´ë¯¸ì§€)ë§Œìœ¼ë¡œ ìƒëŒ€ ìœ„ì¹˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.

#### ğŸ§  Map-free relocalizationì€ì€ ì™œ ì–´ë ¤ìš´ê°€?
- ì§€ë„ ì—†ì´ ì ˆëŒ€ ìœ„ì¹˜ë‚˜ ìŠ¤ì¼€ì¼ì„ ì•Œ ìˆ˜ ì—†ìŒ
- ì´ë¯¸ì§€ ê°„ ëŒ€ì‘ì ë§Œìœ¼ë¡œ metric scaleì˜ ì •í™•í•œ ìœ„ì¹˜ ì¶”ì •ì´ ì–´ë ¤ì›€
- ë‹¤ì–‘í•œ ì‹œì  ë³€í™”(viewpoint change) ì— ê°•ì¸í•´ì•¼ í•¨

#### MASt3R -> dense image matching & metric scale 3D pose estimation
- MASt3RëŠ” dense image matchingê³¼ metric-scale 3D ì¶”ì •ì„ ë™ì‹œì— ë‹¬ì„±í•˜ì—¬, map ì—†ì´ë„ ì •í™•í•œ pose estimationì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
- ì¦‰, 3D geometry ì—†ì´ë„ ì´ë¯¸ì§€ ê°„ì˜ ëŒ€ì‘ì ê³¼ ì¹´ë©”ë¼ í¬ì¦ˆë¥¼ ì¶”ì •í•  ìˆ˜ ìˆì–´ Map-free relocalizationì— ì í•©í•©ë‹ˆë‹¤.

#### ğŸ§­ ê¸°ì¡´ relocalizationê³¼ì˜ ì°¨ì´
- Map-based Relocalization:	ê¸°ì¡´ 3D ëª¨ë¸ ë˜ëŠ” í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í•„ìš”
- Map-free Relocalization:	ì‚¬ì „ ì§€ë„ ì—†ì´ ì´ë¯¸ì§€ ê°„ ëŒ€ì‘ë§Œìœ¼ë¡œ í¬ì¦ˆ ì¶”ì •
